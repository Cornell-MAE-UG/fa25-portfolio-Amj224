---
layout: project
title: MAE 4300 Engineers in Society
description: Final Portfolio
technologies: [N/A]
image: /assets/images/Boeing737Max.jpg
---

The Boeing 737 MAX case demonstrates how ethical failure in engineering rarely comes from a single decision, but instead from a sequence of choices that individually appear reasonable yet collectively produce catastrophic risk. At the center of the case was MCAS, a system introduced to manage altered flight characteristics after engine placement changes. MCAS relied on a single angle of attack (AoA) sensor, could repeatedly command nose down trim, and lacked robust redundancy or clear cockpit indications. Pilots were not explicitly informed of the system’s existence, and key alerts were optional rather than standard. These design decisions embedded assumptions about pilot recognition and response that were optimistic in theory and dangerous in practice, particularly in a safety critical system where failure modes must be anticipated rather than minimized.

These technical vulnerabilities cannot be separated from the organizational environment in which they were created. Boeing was operating under intense competitive pressure to match Airbus while preserving the 737’s “derivative” status, which allowed for faster certification and reduced pilot retraining requirements. Within this context, engineering decisions were shaped by cost, schedule, and contractual obligations. Engineers worked in a fragmented structure where safety information was distributed across teams, simulator data and pilot feedback were not centralized, and dissent was discouraged by hierarchy. Even when concerns were raised, they were often absorbed into internal processes without triggering meaningful design changes, illustrating how organizational structure can quietly suppress ethical action.

Regulatory delegation further complicated accountability. The FAA’s reliance on Boeing employees through the Organization Designation Authorization blurred the boundary between regulator and regulated. Engineers could reasonably believe that internal compliance satisfied external safety obligations, while regulators assumed Boeing’s internal reviews were sufficiently rigorous. This diffusion of responsibility weakened independent oversight and allowed safety concerns to remain internal rather than publicly examined. Ethically, the problem was not merely regulatory capture but a system that diluted individual responsibility to the point where no single actor felt empowered or obligated to intervene decisively.

The decision not to include MCAS in pilot manuals or training highlights how these pressures converged. Boeing characterized MCAS as a minor background function rather than a novel control system, preserving claims of commonality and avoiding simulator training costs for airlines. This framing prioritized market competitiveness over meaningful transparency. While the decision may have met minimum disclosure standards, it failed ethically by depriving pilots of the knowledge necessary to diagnose and respond to abnormal aircraft behavior. Transparency, when defined narrowly as legal compliance rather than informed understanding, becomes an ethical liability rather than a safeguard.

After the first crash, the ethical stakes intensified. Evidence pointed toward MCAS involvement and known system vulnerabilities, yet the fleet was not immediately grounded. Organizational hesitation, uncertainty over authority, and reputational concerns delayed decisive action. Engineers faced conflicts between loyalty to employer and duty to public safety, while leadership weighed economic consequences against precaution. The second crash demonstrated the cost of delay, revealing that the question of grounding was not simply technical but moral: whether uncertainty should default toward protecting human life or preserving institutional stability.

All together, the analyses we did in weeks 6-9, reveal that ethical engineering cannot rely solely on individual integrity. Even competent, well intentioned engineers can produce unsafe outcomes when embedded in systems that reward speed, silence dissent, and diffuse accountability. Preventing similar failures requires structural change: empowering engineers to escalate concerns without fear, redefining organizational incentives to prioritize long term safety, and restoring independent regulatory oversight. The 737 MAX case matters because it shows that safety is not guaranteed by technical skill alone. It depends on how institutions frame risk, define responsibility, and decide whose understanding truly matters.

