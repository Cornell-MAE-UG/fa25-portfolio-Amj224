---
layout: project
title: MAE 4300 Engineers in Society
description: Final Portfolio
image: /assets/images/Boeing737Max.jpg
---


The 737 MAX crashes in Indonesia (2018) and Ethiopia (2019) weren’t just a “bad software” story; they were the product of choices that stacked across design, training, certification, and business pressure. MCAS existed to smooth out handling differences after airframe changes, but it leaned on a single angle-of-attack input, could move the stabilizer repeatedly, and wasn’t fully described to pilots—design moves that only make sense if you assume crews will diagnose and counter a surprise trim event quickly and correctly in real time. Independent reviews later flagged those very assumptions about pilot recognition and response as fragile, recommending better validation methods and more realistic human-factors testing. That’s the core reason we care about “how” decisions were made: the system embedded optimistic bets about human performance and transparency, and then treated them as if they were facts. 


The information and incentive setup helped push the program toward brittle outcomes. People with different roles held different parts of the picture—Boeing teams, FAA staff, airlines, pilots—and the process for a “derivative” model made it easier to focus on changes in isolation instead of interactions (like how AOA input and MCAS logic could combine). Delegating significant certification work to the manufacturer (ODA) isn’t inherently unsafe, but the way it was practiced here created blind spots and misunderstandings inside the regulator about what MCAS could do and how it might fail. The House investigation describes this as a chain of missed opportunities; the DOT Inspector General details gaps in FAA guidance and coordination that allowed key hazards to be underestimated. The point isn’t to relitigate every step—it’s to show that structure (who knows what, who checks whom, and on what timeline) quietly shapes safety just as much as code and metal.


After the accidents, authorities grounded the fleet and worked through a long return-to-service process that required changes well beyond a quick patch. The FAA’s summary explains the package: MCAS was redesigned with multiple safeguards, additional AOA validation and comparators, limits on command authority, and clearer procedures and training for crews. The FAA also updated its Flight Standardization Board guidance and emphasized making failure modes legible in the cockpit—concrete steps that respond directly to the earlier “optimistic assumptions about pilots under stress” critique. These changes are evidence for the larger claim: if your design depends on perfect human response to surprise automation, you haven’t built resilience—you’ve built a trap. 


The accident reports reinforce what went wrong at the sharp end. The Ethiopian final report calls out MCAS’s reliance on a single AOA sensor and the resulting uncommanded trim as central to the event chain, while broader reviews stressed that multiple alerts and cues can overwhelm pilots and slow recognition—exactly the kind of scenario the certification analyses didn’t fully model. This is why human-in-the-loop can’t just be a slogan: the loop has to be understandable under pressure, the system has to advertise its state, and failure modes have to be recoverable with the training pilots actually receive, not the training we wish they had. 


Even years later, ripple effects keep showing how culture and oversight matter. Follow-on quality events—like the MAX-9 door-plug blowout investigation—point to documentation gaps and training/oversight weaknesses that regulators say they’re now tightening through increased scrutiny and production limits; the FAA has also signaled further alerting and AOA-synthesis upgrades on forthcoming MAX variants. The lesson for engineers isn’t “software bad, regulators worse.” It’s that incentives, documentation discipline, and independent checks are part of the technical system. If those parts are weak, the airplane is weak. 


So what do we do differently? First, design for legibility and bounded authority: multiple independent inputs for safety-critical functions, hard limits on automated trim, and interfaces that surface exactly what the system is doing when it matters. Those are not abstractions; they’re precisely the kinds of changes the FAA required for ungrounding. Second, test assumptions about people as aggressively as you test hardware: simulate cluttered alerts, time pressure, and incomplete knowledge; use those results to set training and documentation, not the other way around. That’s the through-line of the NTSB’s recommendations on validating pilot-response assumptions. Third, treat process architecture as design: keep meaningful independence in review, make cross-functional hazards visible early, and align rewards so that raising a safety concern is career-safe, not career-risky. Those steps map directly to the JATR and congressional findings on derivative certification and oversight. 


If there’s a personal takeaway from this case, it’s that courage is part of competence. When a design only works if everyone in the loop behaves perfectly, that’s not a clever optimization—it’s a red flag. And when organizational momentum makes it hard to voice that, the difficulty is a signal, not an excuse. The 737MAX story shows how small, reasonable-sounding shortcuts can add up to systemic risk; the path forward is to “engineer the setup” as carefully as we engineer the system. That means redundancy where it matters, transparency when it counts, and incentives that reward speaking up over shipping fast. The industry’s own corrective actions—software changes, training updates, certification reforms—show that these aren’t theoretical ideals; they’re the practical fixes that actually reduce risk when stakes are measured in lives. 



